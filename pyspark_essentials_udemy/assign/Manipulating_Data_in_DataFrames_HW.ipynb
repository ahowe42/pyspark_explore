{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Data in DataFrames HW\n",
    "\n",
    "\n",
    "#### Let's get started applying what we learned in the lecure!\n",
    "\n",
    "I've provided several questions below to help test and expand you knowledge from the code along lecture. So let's see what you've got!\n",
    "\n",
    "First create your spark instance as we need to do at the start of every project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *#avg, count, expr\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 cores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.150.128:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>manipulateAss</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe4ec2e1730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.appName = 'manipulateAss'\n",
    "# show the number of cores\n",
    "print('%d cores'%spark._jsc.sc().getExecutorMemoryStatus().keySet().size())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our Republican vs. Democrats Tweet DataFrame\n",
    "\n",
    "Attached to the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fil = '../../data/Rep_vs_Dem_tweets.csv'\n",
    "schem = StructType([StructField('Party', StringType()),\n",
    "                    StructField('Handle', StringType()),\n",
    "                    StructField('Tweet', StringType())])\n",
    "tweets = spark.read.format('csv').options(header=True).schema(schem).load(fil).repartition('Party')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataframe\n",
    "\n",
    "Extracted tweets from all of the representatives (latest 200 as of May 17th 2018)\n",
    "\n",
    "**Source:** https://www.kaggle.com/kapastor/democratvsrepublicantweets#ExtractedTweets.csv\n",
    "\n",
    "Use either .show() or .toPandas() check out the first view rows of the dataframe to get an idea of what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92489 records\n",
      "+------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------+\n",
      "|Party                                                                                                                   |Handle                                                                                |Tweet                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------+\n",
      "|investigat‚Ä¶\"                                                                                                            |null                                                                                  |null                                         |\n",
      "|Pruitt‚Äôs Chief Of Staff tried to ‚Äúscreen‚Äù materials before EPA investigators could review them‚Ä¶ https://t.co/2SsE77Tdqt\"|null                                                                                  |null                                         |\n",
      "|05/03/‚Ä¶ https://t.co/JnyNYcgua2\"                                                                                        |null                                                                                  |null                                         |\n",
      "|NOTE that‚Ä¶ https://t.co/HZYoiWg6bb\"                                                                                     |null                                                                                  |null                                         |\n",
      "|MC Lyte - Poor Georgie‚Ä¶ https://t.co/2zSeTQTzMK\"                                                                        |null                                                                                  |null                                         |\n",
      "| Start- November 1                                                                                                      |null                                                                                  |null                                         |\n",
      "|We must fight back agains‚Ä¶\"                                                                                             |null                                                                                  |null                                         |\n",
      "|Happy #Friday                                                                                                           | folks ‚úåüèΩ https://t.co/PCswSl1qug\"                                                   |null                                         |\n",
      "|my Congressman. He fights for workers and does important work on the Transportation and Inf‚Ä¶\"                           |null                                                                                  |null                                         |\n",
      "|#TrumpBudget woul‚Ä¶ https://t.co/TY6E6f0n1s\"                                                                             |null                                                                                  |null                                         |\n",
      "|https://t.co/GI7YnM6nm4\"                                                                                                |null                                                                                  |null                                         |\n",
      "|America will be gre‚Ä¶ https://t.co/iv5ghHN1KQ\"                                                                           |null                                                                                  |null                                         |\n",
      "|According to @CommerceGov                                                                                               | real disposable income has had the biggest gain since April‚Ä¶ https://t.co/nA8Ki3o71q\"|null                                         |\n",
      "|https://t.co/jJzEYuuMMj\"                                                                                                |null                                                                                  |null                                         |\n",
      "|and H.R. 3312 - Systemic Risk Designation Impro‚Ä¶ https://t.co/rGidEsJtAc\"                                               |null                                                                                  |null                                         |\n",
      "|https://t.co/zykO9PRZ81\"                                                                                                |null                                                                                  |null                                         |\n",
      "|To all who are celebrating Passover                                                                                     | may your holiday be joyous and blessed with goo‚Ä¶ https://t.co/9ALHOt12jm\"            |null                                         |\n",
      "|The Kennedy family after Easter services in Palm Beach                                                                  | Florida                                                                              | 1963. More photos: https://t.co/ZqHKZSS47Y‚Ä¶\"|\n",
      "|Glad‚Ä¶ https://t.co/zuus9L32cj\"                                                                                          |null                                                                                  |null                                         |\n",
      "|The U.S. should be laying the groundwork for peac‚Ä¶\"                                                                     |null                                                                                  |null                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# talk\n",
    "print('%d records'%tweets.count())\n",
    "display(tweets.show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prevent Truncation of view**\n",
    "\n",
    "If the view you produced above truncated some of the longer tweets, see if you can prevent that so you can read the whole tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print Schema**\n",
    "\n",
    "First, check the schema to make sure the datatypes are accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need, as I set the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Can you identify any tweet that mentions the handle @LatinoLeader using regexp_extract?\n",
    "\n",
    "It doesn't matter how you identify the row, any identifier will do. You can test your script on row 5 from this dataset. That row contains @LatinoLeader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Party   |Handle       |Tweet                                                                                                                                       |seld|\n",
      "+--------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Democrat|RepDarrenSoto|RT @NALCABPolicy: Meeting with @RepDarrenSoto . Thanks for taking the time to meet with @LatinoLeader ED Marucci Guzman. #NALCABPolicy2018.‚Ä¶|true|\n",
      "+--------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select('*', col('Tweet').like('%@LatinoLeader%').alias('seld')).where(col('seld')==True).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Replace any value other than 'Democrate' or 'Republican' with 'Other' in the Party column.\n",
    "\n",
    "We can see from the output below, that there are several other values other than 'Democrate' or 'Republican' in the Part column. We are assuming that this is dirty data that needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     Party|count|\n",
      "+----------+-----+\n",
      "|Republican|44392|\n",
      "|  Democrat|42068|\n",
      "|     Other| 6029|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.select(when(col('Party').isin(['Democrat', 'Republican']), col('Party')).otherwise('Other').alias('Party'), 'Handle', 'Tweet')\n",
    "\n",
    "tweets.groupBy(col('Party')).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delete all embedded links (ie. \"https:....)\n",
    "\n",
    "For example see the first row in the tweets dataframe. \n",
    "\n",
    "*Note: this may require an google search :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Party     |Handle                                                          |Tweet                                                                                                                                       |\n",
      "+----------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Other     | Florida                                                        |1963. More photos:                                                                                                                          |\n",
      "|Other     | I will join @alisonstewart on @WNYCMidday to discuss my novel  |‚ÄúBig Guns.‚Äù                                                                                                                                 |\n",
      "|Other     | of course                                                      |to‚Ä¶                                                                                                                                         |\n",
      "|Other     | in Vienna                                                      |Friedrich Hayek was born. He would go on to write about economi‚Ä¶\"                                                                           |\n",
      "|Other     | Georgia                                                        |and two deaths have bee‚Ä¶                                                                                                                    |\n",
      "|Other     | more importantly                                               |good citizens                                                                                                                               |\n",
      "|Other     | better #tax code                                               |fewer‚Ä¶                                                                                                                                      |\n",
      "|Other     | 10am to 2pm                                                    |at the Valley Visitor Center                                                                                                                |\n",
      "|Other     | March 27                                                       |6:00pm‚Äì7:00pm                                                                                                                               |\n",
      "|Other     | @rarediseaseday                                                |‚Ä¶                                                                                                                                           |\n",
      "|Other     | the initial claims for state unemployment benefits fell to 209k|‚Ä¶                                                                                                                                           |\n",
      "|Other     | the initial claims for state unemployment benefits fell to 209k|the‚Ä¶\"                                                                                                                                       |\n",
      "|Other     | a survivor of communist China                                  |tells @MKibbe her harrowing story of how Chairman Mao's gove‚Ä¶\"                                                                              |\n",
      "|Other     | the focus seems to be on grandparents                          |mothers                                                                                                                                     |\n",
      "|Republican|RepRalphNorman                                                  |It's #WastefulWednesday!                                                                                                                    |\n",
      "|Republican|RepRalphNorman                                                  |Today we honored all the heroic men &amp; women of law enforcement who lost their lives on the line of duty at the Nati‚Ä¶                    |\n",
      "|Republican|RepRalphNorman                                                  |RT @CongressmanRaja: Last week, @RepRalphNorman  and I hosted a briefing on the economic benefits of solar energy production through the So‚Ä¶|\n",
      "|Republican|RepRalphNorman                                                  |RT @TegaCayPD: Chief Parker was thankful to receive recognition from @RepRalphNorman delivered by Mayor @DavidLONeal for National Police We‚Ä¶|\n",
      "|Republican|RepRalphNorman                                                  |I visited the SC Highway Patrol to bring them cupcakes &amp; thank them for their service. As we honor‚Ä¶                                     |\n",
      "|Republican|RepRalphNorman                                                  |RT @HouseGOP: America‚Äôs economy is booming thanks to pro-growth policies like the #TaxCutsandJobsAct.                                       |\n",
      "+----------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.select('Party', 'Handle', trim(substring_index(col('Tweet'), 'https', 1)).alias('Tweet'))\\\n",
    "    .where(col('Tweet').isNotNull() & col('Handle').isNotNull())\n",
    "tweets.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove any leading or trailing white space in the tweet column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already did that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rename the 'Party' column to 'Dem_Rep'\n",
    "\n",
    "No real reason here :) just wanted you to get practice doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Dem_Rep_Other|Handle                                                          |Tweet                                                                                                                                       |\n",
      "+-------------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Other        | Florida                                                        |1963. More photos:                                                                                                                          |\n",
      "|Other        | I will join @alisonstewart on @WNYCMidday to discuss my novel  |‚ÄúBig Guns.‚Äù                                                                                                                                 |\n",
      "|Other        | of course                                                      |to‚Ä¶                                                                                                                                         |\n",
      "|Other        | in Vienna                                                      |Friedrich Hayek was born. He would go on to write about economi‚Ä¶\"                                                                           |\n",
      "|Other        | Georgia                                                        |and two deaths have bee‚Ä¶                                                                                                                    |\n",
      "|Other        | more importantly                                               |good citizens                                                                                                                               |\n",
      "|Other        | better #tax code                                               |fewer‚Ä¶                                                                                                                                      |\n",
      "|Other        | 10am to 2pm                                                    |at the Valley Visitor Center                                                                                                                |\n",
      "|Other        | March 27                                                       |6:00pm‚Äì7:00pm                                                                                                                               |\n",
      "|Other        | @rarediseaseday                                                |‚Ä¶                                                                                                                                           |\n",
      "|Other        | the initial claims for state unemployment benefits fell to 209k|‚Ä¶                                                                                                                                           |\n",
      "|Other        | the initial claims for state unemployment benefits fell to 209k|the‚Ä¶\"                                                                                                                                       |\n",
      "|Other        | a survivor of communist China                                  |tells @MKibbe her harrowing story of how Chairman Mao's gove‚Ä¶\"                                                                              |\n",
      "|Other        | the focus seems to be on grandparents                          |mothers                                                                                                                                     |\n",
      "|Republican   |RepRalphNorman                                                  |It's #WastefulWednesday!                                                                                                                    |\n",
      "|Republican   |RepRalphNorman                                                  |Today we honored all the heroic men &amp; women of law enforcement who lost their lives on the line of duty at the Nati‚Ä¶                    |\n",
      "|Republican   |RepRalphNorman                                                  |RT @CongressmanRaja: Last week, @RepRalphNorman  and I hosted a briefing on the economic benefits of solar energy production through the So‚Ä¶|\n",
      "|Republican   |RepRalphNorman                                                  |RT @TegaCayPD: Chief Parker was thankful to receive recognition from @RepRalphNorman delivered by Mayor @DavidLONeal for National Police We‚Ä¶|\n",
      "|Republican   |RepRalphNorman                                                  |I visited the SC Highway Patrol to bring them cupcakes &amp; thank them for their service. As we honor‚Ä¶                                     |\n",
      "|Republican   |RepRalphNorman                                                  |RT @HouseGOP: America‚Äôs economy is booming thanks to pro-growth policies like the #TaxCutsandJobsAct.                                       |\n",
      "+-------------+----------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.withColumnRenamed('Party', 'Dem_Rep_Other').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Concatenate the Party and Handle columns\n",
    "\n",
    "Silly yes... but good practice.\n",
    "\n",
    "pyspark.sql.functions.concat_ws(sep, *cols)[source] <br>\n",
    "Concatenates multiple input string columns together into a single string column, using the given separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Who                                                                   |Tweet                                                                                                                                       |\n",
      "+----------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Other- Florida                                                        |1963. More photos:                                                                                                                          |\n",
      "|Other- I will join @alisonstewart on @WNYCMidday to discuss my novel  |‚ÄúBig Guns.‚Äù                                                                                                                                 |\n",
      "|Other- of course                                                      |to‚Ä¶                                                                                                                                         |\n",
      "|Other- in Vienna                                                      |Friedrich Hayek was born. He would go on to write about economi‚Ä¶\"                                                                           |\n",
      "|Other- Georgia                                                        |and two deaths have bee‚Ä¶                                                                                                                    |\n",
      "|Other- more importantly                                               |good citizens                                                                                                                               |\n",
      "|Other- better #tax code                                               |fewer‚Ä¶                                                                                                                                      |\n",
      "|Other- 10am to 2pm                                                    |at the Valley Visitor Center                                                                                                                |\n",
      "|Other- March 27                                                       |6:00pm‚Äì7:00pm                                                                                                                               |\n",
      "|Other- @rarediseaseday                                                |‚Ä¶                                                                                                                                           |\n",
      "|Other- the initial claims for state unemployment benefits fell to 209k|‚Ä¶                                                                                                                                           |\n",
      "|Other- the initial claims for state unemployment benefits fell to 209k|the‚Ä¶\"                                                                                                                                       |\n",
      "|Other- a survivor of communist China                                  |tells @MKibbe her harrowing story of how Chairman Mao's gove‚Ä¶\"                                                                              |\n",
      "|Other- the focus seems to be on grandparents                          |mothers                                                                                                                                     |\n",
      "|Republican-RepRalphNorman                                             |It's #WastefulWednesday!                                                                                                                    |\n",
      "|Republican-RepRalphNorman                                             |Today we honored all the heroic men &amp; women of law enforcement who lost their lives on the line of duty at the Nati‚Ä¶                    |\n",
      "|Republican-RepRalphNorman                                             |RT @CongressmanRaja: Last week, @RepRalphNorman  and I hosted a briefing on the economic benefits of solar energy production through the So‚Ä¶|\n",
      "|Republican-RepRalphNorman                                             |RT @TegaCayPD: Chief Parker was thankful to receive recognition from @RepRalphNorman delivered by Mayor @DavidLONeal for National Police We‚Ä¶|\n",
      "|Republican-RepRalphNorman                                             |I visited the SC Highway Patrol to bring them cupcakes &amp; thank them for their service. As we honor‚Ä¶                                     |\n",
      "|Republican-RepRalphNorman                                             |RT @HouseGOP: America‚Äôs economy is booming thanks to pro-growth policies like the #TaxCutsandJobsAct.                                       |\n",
      "+----------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select(concat_ws('-', col('Party'), col('Handle')).alias('Who'), 'Tweet').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Question\n",
    "\n",
    "Let's image that we want to analyze the hashtags that are used in these tweets. Can you extract all the hashtags you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refa(x):\n",
    "    res = ','.join(re.findall('\\#[A-Za-z0-9]*', x))\n",
    "    return res\n",
    "refaUDF = udf(lambda x: refa(x), ArrayType(StringType(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o765.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 118.0 failed 1 times, most recent failure: Lost task 1.0 in stage 118.0 (TID 1233) (192.168.150.128 executor driver): org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  458961458 (0x1b5b3232)\nPython command to execute the daemon was:\n  ipython3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/jars/spark-core_2.12-3.1.1.jar:/home/ahowe42/anaconda3/bin\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:238)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  458961458 (0x1b5b3232)\nPython command to execute the daemon was:\n  ipython3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/jars/spark-core_2.12-3.1.1.jar:/home/ahowe42/anaconda3/bin\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:238)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-298137c8ad06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# can't get this to match all of the patterns in the tweet :-(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthisPatt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(\\#[A-Za-z0-9]*)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m tweets.select((length(col('Tweet')) - length(array_join(split('Tweet', '#'), ''))).alias('mult'),\n\u001b[0m\u001b[1;32m      4\u001b[0m               regexp_extract(col('Tweet'), thisPatt, 0).alias('Hashtag'), refaUDF('Tweet').alias('Hashtags'), 'Tweet')\\\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mult'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o765.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 118.0 failed 1 times, most recent failure: Lost task 1.0 in stage 118.0 (TID 1233) (192.168.150.128 executor driver): org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  458961458 (0x1b5b3232)\nPython command to execute the daemon was:\n  ipython3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/jars/spark-core_2.12-3.1.1.jar:/home/ahowe42/anaconda3/bin\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:238)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  458961458 (0x1b5b3232)\nPython command to execute the daemon was:\n  ipython3 -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip:/home/ahowe42/spark-3.1.1-bin-hadoop2.7/jars/spark-core_2.12-3.1.1.jar:/home/ahowe42/anaconda3/bin\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:238)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:70)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# can't get this to match all of the patterns in the tweet because of the unexplained errror :-(\n",
    "thisPatt = '(\\#[A-Za-z0-9]*)'\n",
    "tweets.select((length(col('Tweet')) - length(array_join(split('Tweet', '#'), ''))).alias('mult'),\n",
    "              regexp_extract(col('Tweet'), thisPatt, 0).alias('Hashtag'), refaUDF('Tweet').alias('Hashtags'), 'Tweet')\\\n",
    "    .orderBy(col('mult').desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create our own dataset to work with real dates\n",
    "\n",
    "This is a dataset of patient visits from a medical office. It contains the patients first and last names, date of birth, and the dates of their first 3 visits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+---------+---------+\n",
      "|first_name|last_name|       dob|  visit1|   visit2|   visit3|\n",
      "+----------+---------+----------+--------+---------+---------+\n",
      "|  Mohammed|   Alfasy|  1987-4-8|2016-1-7| 2017-2-3| 2018-3-2|\n",
      "|     Marcy|Wellmaker|  1986-4-8|2015-1-7| 2017-1-3| 2018-1-2|\n",
      "|     Ginny|   Ginger| 1986-7-10|2014-8-7| 2015-2-3| 2016-3-2|\n",
      "|     Vijay| Doberson|  1988-5-2|2016-1-7| 2018-2-3| 2018-3-2|\n",
      "|     Orhan|  Gelicek| 1987-5-11|2016-5-7| 2017-1-3| 2018-9-2|\n",
      "|     Sarah|    Jones|  1956-7-6|2016-4-7| 2017-8-3|2018-10-2|\n",
      "|      John|  Johnson|2017-10-12|2018-1-2|2018-10-3| 2018-3-2|\n",
      "+----------+---------+----------+--------+---------+---------+\n",
      "\n",
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- visit1: string (nullable = true)\n",
      " |-- visit2: string (nullable = true)\n",
      " |-- visit3: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''from pyspark.sql.types import *\n",
    "\n",
    "md_office = [('Mohammed','Alfasy','1987-4-8','2016-1-7','2017-2-3','2018-3-2') \\\n",
    "            ,('Marcy','Wellmaker','1986-4-8','2015-1-7','2017-1-3','2018-1-2') \\\n",
    "            ,('Ginny','Ginger','1986-7-10','2014-8-7','2015-2-3','2016-3-2') \\\n",
    "            ,('Vijay','Doberson','1988-5-2','2016-1-7','2018-2-3','2018-3-2') \\\n",
    "            ,('Orhan','Gelicek','1987-5-11','2016-5-7','2017-1-3','2018-9-2') \\\n",
    "            ,('Sarah','Jones','1956-7-6','2016-4-7','2017-8-3','2018-10-2') \\\n",
    "            ,('John','Johnson','2017-10-12','2018-1-2','2018-10-3','2018-3-2') ]\n",
    "\n",
    "df = spark.createDataFrame(md_office,['first_name','last_name','dob','visit1','visit2','visit3']) # schema=final_struc'''\n",
    "\n",
    "df = spark.read.format('csv').options(header=True, inferSchema=True).load('../../data/hospitalvisits.csv')\n",
    "\n",
    "# Check to make sure it worked\n",
    "df.show()\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! The dates are still stored as text... let's try converting them again and see if we have any issues this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- visit1: date (nullable = true)\n",
      " |-- visit2: date (nullable = true)\n",
      " |-- visit3: date (nullable = true)\n",
      "\n",
      "+----------+---------+----------+----------+----------+----------+\n",
      "|first_name|last_name|       dob|    visit1|    visit2|    visit3|\n",
      "+----------+---------+----------+----------+----------+----------+\n",
      "|  Mohammed|   Alfasy|1987-04-08|2016-01-07|2017-02-03|2018-03-02|\n",
      "|     Marcy|Wellmaker|1986-04-08|2015-01-07|2017-01-03|2018-01-02|\n",
      "|     Ginny|   Ginger|1986-07-10|2014-08-07|2015-02-03|2016-03-02|\n",
      "|     Vijay| Doberson|1988-05-02|2016-01-07|2018-02-03|2018-03-02|\n",
      "|     Orhan|  Gelicek|1987-05-11|2016-05-07|2017-01-03|2018-09-02|\n",
      "|     Sarah|    Jones|1956-07-06|2016-04-07|2017-08-03|2018-10-02|\n",
      "|      John|  Johnson|2017-10-12|2018-01-02|2018-10-03|2018-03-02|\n",
      "+----------+---------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\") # I have *no idea* why this is needed???\n",
    "dtFmt = 'yyyy-M-d'\n",
    "df = df.withColumn('dob', to_date(col('dob'), dtFmt))\\\n",
    "    .withColumn('visit1', to_date(col('visit1'), dtFmt))\\\n",
    "    .withColumn('visit2', to_date(col('visit2'), dtFmt))\\\n",
    "    .withColumn('visit3', to_date(col('visit3'), dtFmt))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Can you calculate a variable showing the length of time between patient visits?\n",
    "\n",
    "Compare visit1 to visit2 and visit2 to visit3 for all patients and see what the average length of time is between visits. Create an alias for it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|avgTime|\n",
      "+-------+\n",
      "|  392.5|\n",
      "|  545.5|\n",
      "|  286.5|\n",
      "|  392.5|\n",
      "|  424.0|\n",
      "|  454.0|\n",
      "|   29.5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(datediff('visit2', 'visit1').alias('time1'), datediff('visit3', 'visit2').alias('time2'))\\\n",
    "    .select(((col('time1')+col('time2'))/2).alias('avgTime')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Can you calculate the age of each patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|       dob|               age|\n",
      "+----------+------------------+\n",
      "|1987-04-08| 34.14794520547945|\n",
      "|1986-04-08| 35.14794520547945|\n",
      "|1986-07-10|34.893150684931506|\n",
      "|1988-05-02| 33.07945205479452|\n",
      "|1987-05-11|34.057534246575344|\n",
      "|1956-07-06| 64.92328767123287|\n",
      "|2017-10-12|3.6136986301369864|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dob', ((datediff(current_date(), 'dob'))/365).alias('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Can you extract the month from the first visit column and call it \"Month\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|    visit1|visit1_Mo|\n",
      "+----------+---------+\n",
      "|2016-01-07|        1|\n",
      "|2015-01-07|        1|\n",
      "|2014-08-07|        8|\n",
      "|2016-01-07|        1|\n",
      "|2016-05-07|        5|\n",
      "|2016-04-07|        4|\n",
      "|2018-01-02|        1|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('visit1', month('visit1').alias('visit1_Mo')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Challenges with working with date and timestamps\n",
    "\n",
    "Let's read in the supermarket sales dataframe attached to the lecture now and see some of the issues that can come up when working with date and timestamps values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "fil = '../../data/supermarket_sales.csv'\n",
    "schem = StructType([StructField('Invoice ID', StringType()), StructField('Branch', StringType()),\n",
    "                    StructField('City', StringType()), StructField('Customer type', StringType()),\n",
    "                    StructField('Gender', StringType()), StructField('Product line', StringType()),\n",
    "                    StructField('Unit price', FloatType()), StructField('Quantity', IntegerType()),\n",
    "                    StructField('Tax 5%', FloatType()), StructField('Total', FloatType()),\n",
    "                    StructField('Date', DateType()), StructField('Time', TimestampType()),\n",
    "                    StructField('Payment', StringType()), StructField('cogs', FloatType()),\n",
    "                    StructField('gross margin percentage', FloatType()), StructField('gross income', FloatType()),\n",
    "                    StructField('Rating', FloatType())])\n",
    "supers = spark.read.format('csv').options(header=True, dateFormat='MM/dd/yyyy', timestampFormat='HH:mm').schema(schem).load(fil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "The growth of supermarkets in most populated cities are increasing and market competitions are also high. The dataset is one of the historical sales of supermarket company which has recorded in 3 different branches for 3 months data. \n",
    "\n",
    " - Attribute information\n",
    " - Invoice id: Computer generated sales slip invoice identification number\n",
    " - Branch: Branch of supercenter (3 branches are available identified by A, B and C).\n",
    " - City: Location of supercenters\n",
    " - Customer type: Type of customers, recorded by Members for customers using member card and Normal for without member card.\n",
    " - Gender: Gender type of customer\n",
    " - Product line: General item categorization groups - Electronic accessories, Fashion accessories, Food and beverages, Health and beauty, Home and lifestyle, Sports and travel\n",
    " - Unit price: Price of each product in USD\n",
    " - Quantity: Number of products purchased by customer\n",
    " - Tax: 5% tax fee for customer buying\n",
    " - Total: Total price including tax\n",
    " - Date: Date of purchase (Record available from January 2019 to March 2019)\n",
    " - Time: Purchase time (10am to 9pm)\n",
    " - Payment: Payment used by customer for purchase (3 methods are available ‚Äì Cash, Credit card and Ewallet)\n",
    " - COGS: Cost of goods sold\n",
    " - Gross margin percentage: Gross margin percentage\n",
    " - Gross income: Gross income\n",
    " - Rating: Customer stratification rating on their overall shopping experience (On a scale of 1 to 10)\n",
    "\n",
    "**Source:** https://www.kaggle.com/aungpyaeap/supermarket-sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View dataframe and schema as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice ID: string (nullable = true)\n",
      " |-- Branch: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Customer type: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Product line: string (nullable = true)\n",
      " |-- Unit price: float (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Tax 5%: float (nullable = true)\n",
      " |-- Total: float (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      " |-- cogs: float (nullable = true)\n",
      " |-- gross margin percentage: float (nullable = true)\n",
      " |-- gross income: float (nullable = true)\n",
      " |-- Rating: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice ID</th>\n",
       "      <th>Branch</th>\n",
       "      <th>City</th>\n",
       "      <th>Customer type</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Product line</th>\n",
       "      <th>Unit price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Tax 5%</th>\n",
       "      <th>Total</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Payment</th>\n",
       "      <th>cogs</th>\n",
       "      <th>gross margin percentage</th>\n",
       "      <th>gross income</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750-67-8428</td>\n",
       "      <td>A</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>74.690002</td>\n",
       "      <td>7</td>\n",
       "      <td>26.141500</td>\n",
       "      <td>548.971497</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>1970-01-01 13:08:00</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>522.830017</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>26.141500</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226-31-3081</td>\n",
       "      <td>C</td>\n",
       "      <td>Naypyitaw</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Female</td>\n",
       "      <td>Electronic accessories</td>\n",
       "      <td>15.280000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>80.220001</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>1970-01-01 10:29:00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>76.400002</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>631-41-3108</td>\n",
       "      <td>A</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Male</td>\n",
       "      <td>Home and lifestyle</td>\n",
       "      <td>46.330002</td>\n",
       "      <td>7</td>\n",
       "      <td>16.215500</td>\n",
       "      <td>340.525513</td>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>1970-01-01 13:23:00</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>324.309998</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>16.215500</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123-19-1176</td>\n",
       "      <td>A</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Male</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>58.220001</td>\n",
       "      <td>8</td>\n",
       "      <td>23.288000</td>\n",
       "      <td>489.048004</td>\n",
       "      <td>2019-01-27</td>\n",
       "      <td>1970-01-01 20:33:00</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>465.760010</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>23.288000</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373-73-7910</td>\n",
       "      <td>A</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sports and travel</td>\n",
       "      <td>86.309998</td>\n",
       "      <td>7</td>\n",
       "      <td>30.208500</td>\n",
       "      <td>634.378479</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>1970-01-01 10:37:00</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>604.169983</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>30.208500</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>699-14-3026</td>\n",
       "      <td>C</td>\n",
       "      <td>Naypyitaw</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Male</td>\n",
       "      <td>Electronic accessories</td>\n",
       "      <td>85.389999</td>\n",
       "      <td>7</td>\n",
       "      <td>29.886499</td>\n",
       "      <td>627.616516</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>1970-01-01 18:30:00</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>597.729980</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>29.886499</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>355-53-5943</td>\n",
       "      <td>A</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Electronic accessories</td>\n",
       "      <td>68.839996</td>\n",
       "      <td>6</td>\n",
       "      <td>20.652000</td>\n",
       "      <td>433.691986</td>\n",
       "      <td>2019-02-25</td>\n",
       "      <td>1970-01-01 14:36:00</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>413.040009</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>20.652000</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>315-22-5665</td>\n",
       "      <td>C</td>\n",
       "      <td>Naypyitaw</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Female</td>\n",
       "      <td>Home and lifestyle</td>\n",
       "      <td>73.559998</td>\n",
       "      <td>10</td>\n",
       "      <td>36.779999</td>\n",
       "      <td>772.380005</td>\n",
       "      <td>2019-02-24</td>\n",
       "      <td>1970-01-01 11:38:00</td>\n",
       "      <td>Ewallet</td>\n",
       "      <td>735.599976</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>36.779999</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>665-32-9167</td>\n",
       "      <td>A</td>\n",
       "      <td>Yangon</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Health and beauty</td>\n",
       "      <td>36.259998</td>\n",
       "      <td>2</td>\n",
       "      <td>3.626000</td>\n",
       "      <td>76.146004</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>1970-01-01 17:15:00</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>72.519997</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>3.626000</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>692-92-5582</td>\n",
       "      <td>B</td>\n",
       "      <td>Mandalay</td>\n",
       "      <td>Member</td>\n",
       "      <td>Female</td>\n",
       "      <td>Food and beverages</td>\n",
       "      <td>54.840000</td>\n",
       "      <td>3</td>\n",
       "      <td>8.226000</td>\n",
       "      <td>172.746002</td>\n",
       "      <td>2019-02-20</td>\n",
       "      <td>1970-01-01 13:27:00</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>164.520004</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>8.226000</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Invoice ID Branch       City Customer type  Gender  \\\n",
       "0  750-67-8428      A     Yangon        Member  Female   \n",
       "1  226-31-3081      C  Naypyitaw        Normal  Female   \n",
       "2  631-41-3108      A     Yangon        Normal    Male   \n",
       "3  123-19-1176      A     Yangon        Member    Male   \n",
       "4  373-73-7910      A     Yangon        Normal    Male   \n",
       "5  699-14-3026      C  Naypyitaw        Normal    Male   \n",
       "6  355-53-5943      A     Yangon        Member  Female   \n",
       "7  315-22-5665      C  Naypyitaw        Normal  Female   \n",
       "8  665-32-9167      A     Yangon        Member  Female   \n",
       "9  692-92-5582      B   Mandalay        Member  Female   \n",
       "\n",
       "             Product line  Unit price  Quantity     Tax 5%       Total  \\\n",
       "0       Health and beauty   74.690002         7  26.141500  548.971497   \n",
       "1  Electronic accessories   15.280000         5   3.820000   80.220001   \n",
       "2      Home and lifestyle   46.330002         7  16.215500  340.525513   \n",
       "3       Health and beauty   58.220001         8  23.288000  489.048004   \n",
       "4       Sports and travel   86.309998         7  30.208500  634.378479   \n",
       "5  Electronic accessories   85.389999         7  29.886499  627.616516   \n",
       "6  Electronic accessories   68.839996         6  20.652000  433.691986   \n",
       "7      Home and lifestyle   73.559998        10  36.779999  772.380005   \n",
       "8       Health and beauty   36.259998         2   3.626000   76.146004   \n",
       "9      Food and beverages   54.840000         3   8.226000  172.746002   \n",
       "\n",
       "         Date                Time      Payment        cogs  \\\n",
       "0  2019-01-05 1970-01-01 13:08:00      Ewallet  522.830017   \n",
       "1  2019-03-08 1970-01-01 10:29:00         Cash   76.400002   \n",
       "2  2019-03-03 1970-01-01 13:23:00  Credit card  324.309998   \n",
       "3  2019-01-27 1970-01-01 20:33:00      Ewallet  465.760010   \n",
       "4  2019-02-08 1970-01-01 10:37:00      Ewallet  604.169983   \n",
       "5  2019-03-25 1970-01-01 18:30:00      Ewallet  597.729980   \n",
       "6  2019-02-25 1970-01-01 14:36:00      Ewallet  413.040009   \n",
       "7  2019-02-24 1970-01-01 11:38:00      Ewallet  735.599976   \n",
       "8  2019-01-10 1970-01-01 17:15:00  Credit card   72.519997   \n",
       "9  2019-02-20 1970-01-01 13:27:00  Credit card  164.520004   \n",
       "\n",
       "   gross margin percentage  gross income  Rating  \n",
       "0                 4.761905     26.141500     9.1  \n",
       "1                 4.761905      3.820000     9.6  \n",
       "2                 4.761905     16.215500     7.4  \n",
       "3                 4.761905     23.288000     8.4  \n",
       "4                 4.761905     30.208500     5.3  \n",
       "5                 4.761905     29.886499     4.1  \n",
       "6                 4.761905     20.652000     5.8  \n",
       "7                 4.761905     36.779999     8.0  \n",
       "8                 4.761905      3.626000     7.2  \n",
       "9                 4.761905      8.226000     5.9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "supers.printSchema()\n",
    "display(supers.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date field to date type\n",
    "\n",
    "Looks like we need to convert the date field into a date type. Let's go ahead and do that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we extract the month value from the date field?\n",
    "\n",
    "If you had trouble converting the date field in the previous question think about a more creative solution to extract the month from that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0 Working with Arrays\n",
    "\n",
    "Here is a dataframe of reviews from the movie the Dark Night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- review_txt: string (nullable = true)\n",
      "\n",
      "+------+--------------------------------------------------------------------------------------+\n",
      "|rating|review_txt                                                                            |\n",
      "+------+--------------------------------------------------------------------------------------+\n",
      "|5     |Epic. This is the best movie I have EVER seen                                         |\n",
      "|4     |Pretty good                                                                           |\n",
      "|3     |So so. Casting could have been improved                                               |\n",
      "|5     |The most EPIC movie of the year! Casting was awesome. Special effects were so intense.|\n",
      "|4     |Solid but I would have liked to see more of the love story                            |\n",
      "|5     |THE BOMB!!!!!!!                                                                       |\n",
      "+------+--------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''from pyspark.sql.functions import *\n",
    "\n",
    "values = [(5,'Epic. This is the best movie I have EVER seen'), \\\n",
    "          (4,'Pretty good, but I would have liked to seen better special effects'), \\\n",
    "          (3,'So so. Casting could have been improved'), \\\n",
    "          (5,'The most EPIC movie of the year! Casting was awesome. Special effects were so intense.'), \\\n",
    "          (4,'Solid but I would have liked to see more of the love story'), \\\n",
    "          (5,'THE BOMB!!!!!!!')]\n",
    "reviews = spark.createDataFrame(values,['rating', 'review_txt'])'''\n",
    "\n",
    "reviews = spark.read.format('csv').options(header=True, inferSchema=True).load('../../data/reviews.csv')\n",
    "\n",
    "reviews.printSchema()\n",
    "reviews.show(6,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Let's see if we can create an array off of the review text column and then derive some meaningful results from it.\n",
    "\n",
    "**But first** we need to clean the rview_txt column to make sure we can get what we need from our analysis later on. So let's do the following:\n",
    "\n",
    "1. Remove all punctuation\n",
    "2. lower case everything\n",
    "3. Remove white space (trim)\n",
    "3. Then finally, split the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "|rating|review_txt                                                                            |words                                                                                              |\n",
      "+------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "|5     |Epic. This is the best movie I have EVER seen                                         |[epic, this, is, the, best, movie, i, have, ever, seen]                                            |\n",
      "|4     |Pretty good                                                                           |[pretty, good]                                                                                     |\n",
      "|3     |So so. Casting could have been improved                                               |[so, so, casting, could, have, been, improved]                                                     |\n",
      "|5     |The most EPIC movie of the year! Casting was awesome. Special effects were so intense.|[the, most, epic, movie, of, the, year, casting, was, awesome, special, effects, were, so, intense]|\n",
      "|4     |Solid but I would have liked to see more of the love story                            |[solid, but, i, would, have, liked, to, see, more, of, the, love, story]                           |\n",
      "|5     |THE BOMB!!!!!!!                                                                       |[the, bomb]                                                                                        |\n",
      "+------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = reviews.select('*', split(regexp_replace(trim(lower('review_txt')), '[^a-z\\s]', ''), ' ').alias('words'))\n",
    "reviews.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Alright now let's see if we can find which reviews contain the word 'Epic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|          review_txt|\n",
      "+------+--------------------+\n",
      "|     5|Epic. This is the...|\n",
      "|     5|The most EPIC mov...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.select('rating', 'review_txt').where(array_contains(col('words'), 'epic')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! Great Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
