{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nuclear-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *#avg, count, expr\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "remarkable-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decent-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1391578\n",
      "+-------+-----+---------+------+-----------+\n",
      "|   date|delay|dinstance|origin|destination|\n",
      "+-------+-----+---------+------+-----------+\n",
      "|1161430|   -9|      992|   BOI|        MSP|\n",
      "|1280810|   -8|      464|   EWR|        DAY|\n",
      "|1141210|    0|      706|   AUS|        ATL|\n",
      "|1171120|    9|      407|   CLT|        MCO|\n",
      "|1202030|   20|      824|   DEN|        SJC|\n",
      "|1140755|   -9|      505|   FLL|        ATL|\n",
      "|1192040|   -6|      127|   ATL|        MGM|\n",
      "|1312055|    0|      505|   ATL|        FLL|\n",
      "|1191430|   -5|      127|   ATL|        MGM|\n",
      "|1111545|   36|      352|   DTW|        DCA|\n",
      "|1221734|    5|      126|   ABY|        ATL|\n",
      "|1030905|   -4|      437|   ATL|        LFT|\n",
      "|1242000|    0|     1846|   CLT|        LAX|\n",
      "|1101543|   -7|      805|   CLE|        TPA|\n",
      "|1030630|   -1|      788|   ATL|        MSP|\n",
      "|1271641|    0|      262|   ATL|        MOB|\n",
      "|1041745|   28|      473|   ATL|        PBI|\n",
      "|1210940|    3|      374|   CLT|        CLE|\n",
      "|1040835|   -3|      822|   EWR|        MEM|\n",
      "|1101823|   14|      856|   CMH|        IAH|\n",
      "+-------+-----+---------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "fil = \"./LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "\n",
    "schema = StructType([StructField('date', IntegerType()),\n",
    "                     StructField('delay', IntegerType()),\n",
    "                     StructField('dinstance', IntegerType()),\n",
    "                     StructField('origin', StringType()),\n",
    "                     StructField('destination', StringType())])\n",
    "\n",
    "data = ss.read.format('csv').options(header=True, inferschema=False).schema(schema).load(fil).repartition(8).cache()\n",
    "print(data.rdd.getNumPartitions())\n",
    "print(data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out data to parquet\n",
    "fil = \"./LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays_out\"\n",
    "data.write.format('parquet').mode('overwrite').save(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas and write to parquet\n",
    "fil = \"./LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays_out_pd\"\n",
    "data.toPandas().to_parquet(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now read again from parque\n",
    "fil = \"./LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays_out\"\n",
    "datapq = ss.read.format('parquet').load(fil)\n",
    "datapq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and again read again from parque, but the pandas version\n",
    "fil = \"./LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays_out_pd\"\n",
    "datapqp = ss.read.format('parquet').load(fil)\n",
    "datapqp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-impact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "asian-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-monitor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
