{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thick-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *#avg, count, expr\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prerequisite-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-november",
   "metadata": {},
   "source": [
    "## Access the <a href=http://localhost:4040/jobs/> Spark GUI</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "integral-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "show progress? 0\n"
     ]
    }
   ],
   "source": [
    "shows = int(input('show progress?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "colonial-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-29 12:45:17.963974\n"
     ]
    }
   ],
   "source": [
    "stt = dt.datetime.now()\n",
    "print(stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "serious-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' load the data into temp views '''\n",
    "# paths\n",
    "filDelay = './LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv'\n",
    "filAport = './LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt'\n",
    "\n",
    "# load delay data\n",
    "schema = StructType([StructField('date', StringType()), # need this as a string so it's at least filterable correctly\n",
    "                     StructField('delay', IntegerType()),\n",
    "                     StructField('distance', IntegerType()),\n",
    "                     StructField('origin', StringType()),\n",
    "                     StructField('destination', StringType())])\n",
    "\n",
    "dfDelay = ss.read.format('csv').options(header=True, inferschema=False).schema(schema).load(filDelay).cache()\n",
    "\n",
    "# load the airports data\n",
    "schema = StructType([StructField('City', StringType()),\n",
    "                     StructField('State', StringType()),\n",
    "                     StructField('Country', StringType()),\n",
    "                     StructField('IATA', StringType())])\n",
    "dfAport = ss.read.format('csv').options(header=True, inferschema=False).schema(schema).option(\"delimiter\", \"\\t\").load(filAport).cache()\n",
    "\n",
    "# make views\n",
    "dfDelay.createOrReplaceTempView('departureDelays')\n",
    "dfAport.createOrReplaceTempView('airports_na')\n",
    "\n",
    "# see some data\n",
    "if shows:\n",
    "    ss.sql('select * from departureDelays limit 10;').show()\n",
    "    ss.sql('select * from airports_na limit 10;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stone-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter delays Seattle -> San Francisco\n",
    "foo = dfDelay.filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and date like '01010%' and delay > 0\"\"\")).cache()\n",
    "foo.createOrReplaceTempView('foo')\n",
    "if shows:\n",
    "    ss.sql('select * from foo;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thousand-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate union\n",
    "if shows:\n",
    "    print(dfDelay.distinct().union(foo).count()) # union is UNION ALL\n",
    "    print(dfDelay.distinct().union(foo).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "burning-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join - spark uses broadcast without me specifying it\n",
    "jnd = foo.join(dfAport[['IATA', 'City', 'State']], on=[foo['origin']==dfAport['IATA']], how='inner').withColumnRenamed('City', 'Origin_City').withColumnRenamed('State', 'Origin_State').drop('IATA').\\\n",
    "    join(dfAport[['IATA', 'City', 'State']], on=[foo['destination']==dfAport['IATA']], how='inner').withColumnRenamed('City', 'Dest_City').withColumnRenamed('State', 'Dest_State').drop('IATA')\n",
    "#jnd = foo.join(broadcast(dfAport[['IATA', 'City', 'State']]), on=[foo['origin']==dfAport['IATA']], how='inner').withColumnRenamed('City', 'Origin_City').withColumnRenamed('State', 'Origin_State').drop('IATA').\\\n",
    "#    join(broadcast(dfAport[['IATA', 'City', 'State']]), on=[foo['destination']==dfAport['IATA']], how='inner').withColumnRenamed('City', 'Dest_City').withColumnRenamed('State', 'Dest_State').drop('IATA')\n",
    "if shows:\n",
    "    jnd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ready-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [date#0, delay#1, distance#2, origin#3, destination#4, Origin_City#190, Origin_State#199, City#35 AS Dest_City#248, State#36 AS Dest_State#259]\n",
      "+- *(3) BroadcastHashJoin [destination#4], [IATA#38], Inner, BuildRight\n",
      "   :- *(3) Project [date#0, delay#1, distance#2, origin#3, destination#4, City#35 AS Origin_City#190, State#36 AS Origin_State#199]\n",
      "   :  +- *(3) BroadcastHashJoin [origin#3], [IATA#38], Inner, BuildRight\n",
      "   :     :- *(3) Filter (isnotnull(origin#3) AND isnotnull(destination#4))\n",
      "   :     :  +- InMemoryTableScan [date#0, delay#1, distance#2, origin#3, destination#4], [isnotnull(origin#3), isnotnull(destination#4)]\n",
      "   :     :        +- InMemoryRelation [date#0, delay#1, distance#2, origin#3, destination#4], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :     :              +- *(1) Filter (((((((isnotnull(origin#3) AND isnotnull(destination#4)) AND isnotnull(delay#1)) AND isnotnull(date#0)) AND (origin#3 = SEA)) AND (destination#4 = SFO)) AND StartsWith(date#0, 01010)) AND (delay#1 > 0))\n",
      "   :     :                 +- InMemoryTableScan [date#0, delay#1, distance#2, origin#3, destination#4], [isnotnull(origin#3), isnotnull(destination#4), isnotnull(delay#1), isnotnull(date#0), (origin#3 = SEA), (destination#4 = SFO), StartsWith(date#0, 01010), (delay#1 > 0)]\n",
      "   :     :                       +- InMemoryRelation [date#0, delay#1, distance#2, origin#3, destination#4], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :     :                             +- FileScan csv [date#0,delay#1,distance#2,origin#3,destination#4] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/howean/Documents/spark_learning/LearningSparkV2-master/databrick..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:string,delay:int,distance:int,origin:string,destination:string>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false])), [id=#68]\n",
      "   :        +- *(1) Filter isnotnull(IATA#38)\n",
      "   :           +- InMemoryTableScan [IATA#38, City#35, State#36], [isnotnull(IATA#38)]\n",
      "   :                 +- InMemoryRelation [City#35, State#36, Country#37, IATA#38], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                       +- FileScan csv [City#35,State#36,Country#37,IATA#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/howean/Documents/spark_learning/LearningSparkV2-master/databrick..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Country:string,IATA:string>\n",
      "   +- ReusedExchange [IATA#38, City#35, State#36], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false])), [id=#68]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jnd.explain('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "environmental-riding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.142621\n"
     ]
    }
   ],
   "source": [
    "# outer join  broadcast is slightly faster\n",
    "#mn = dfDelay.select(['origin', 'delay']).filter(col('delay')>0).groupBy(['origin']).mean().withColumnRenamed('origin','org')\n",
    "#mx = dfDelay.select(['origin', 'delay']).filter(col('delay')>0).groupBy(['origin']).max()\n",
    "#mnmx = mn.join(mx, on=[mn['org']==mx['origin']], how='inner').drop('org')\n",
    "\n",
    "timA = dt.datetime.now()\n",
    "mnmx = dfDelay.select(['origin', 'delay']).filter(col('delay')>0).groupBy(['origin']).agg(*[mean(col('delay')).alias('delay_mean'), max(col('delay')).alias('delay_max')])\n",
    "#avgDelay = dfAport.join(mnmx, on=[dfAport['IATA']==mnmx['origin']], how='left_outer').drop('origin')\n",
    "avgDelay = dfAport.join(broadcast(mnmx), on=[dfAport['IATA']==mnmx['origin']], how='left_outer').drop('origin')\n",
    "if shows:\n",
    "    avgDelay.show()\n",
    "timB = dt.datetime.now()\n",
    "print(timB-timA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mobile-groove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [City#35, State#36, Country#37, IATA#38, delay_mean#519, delay_max#521]\n",
      "+- *(3) BroadcastHashJoin [IATA#38], [origin#3], LeftOuter, BuildRight\n",
      "   :- InMemoryTableScan [City#35, State#36, Country#37, IATA#38]\n",
      "   :     +- InMemoryRelation [City#35, State#36, Country#37, IATA#38], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :           +- FileScan csv [City#35,State#36,Country#37,IATA#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/howean/Documents/spark_learning/LearningSparkV2-master/databrick..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Country:string,IATA:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#136]\n",
      "      +- *(2) HashAggregate(keys=[origin#3], functions=[avg(cast(delay#1 as bigint)), max(delay#1)])\n",
      "         +- Exchange hashpartitioning(origin#3, 200), true, [id=#132]\n",
      "            +- *(1) HashAggregate(keys=[origin#3], functions=[partial_avg(cast(delay#1 as bigint)), partial_max(delay#1)])\n",
      "               +- *(1) Filter ((isnotnull(delay#1) AND (delay#1 > 0)) AND isnotnull(origin#3))\n",
      "                  +- InMemoryTableScan [origin#3, delay#1], [isnotnull(delay#1), (delay#1 > 0), isnotnull(origin#3)]\n",
      "                        +- InMemoryRelation [date#0, delay#1, distance#2, origin#3, destination#4], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                              +- FileScan csv [date#0,delay#1,distance#2,origin#3,destination#4] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/howean/Documents/spark_learning/LearningSparkV2-master/databrick..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:string,delay:int,distance:int,origin:string,destination:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgDelay.explain('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "welsh-shore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+------------------+---------+-------+\n",
      "|       City|State|Country|IATA|        delay_mean|delay_max| Status|\n",
      "+-----------+-----+-------+----+------------------+---------+-------+\n",
      "|    Abilene|   TX|    USA| ABI| 50.04494382022472|      395|Delayed|\n",
      "|      Akron|   OH|    USA| CAK| 42.40041067761807|      425|Delayed|\n",
      "|     Albany|   GA|    USA| ABY| 37.09230769230769|      236|Delayed|\n",
      "|     Albany|   NY|    USA| ALB| 36.91700680272109|      491|Delayed|\n",
      "|Albuquerque|   NM|    USA| ABQ|  31.5553772070626|     1305|Delayed|\n",
      "| Alexandria|   LA|    USA| AEX|50.634328358208954|      580|Delayed|\n",
      "|  Allentown|   PA|    USA| ABE|        52.8828125|      333|Delayed|\n",
      "|   Amarillo|   TX|    USA| AMA|38.015068493150686|      624|Delayed|\n",
      "|  Anchorage|   AK|    USA| ANC|30.401273885350317|     1033|Delayed|\n",
      "|   Appleton|   WI|    USA| ATW| 47.13942307692308|      522|Delayed|\n",
      "|  Asheville|   NC|    USA| AVL|46.645348837209305|      332|Delayed|\n",
      "|    Atlanta|   GA|    USA| ATL| 30.63710911351248|      925|Delayed|\n",
      "|    Augusta|   GA|    USA| AGS|39.305263157894736|      294|Delayed|\n",
      "|     Austin|   TX|    USA| AUS|30.045319295033188|     1461|Delayed|\n",
      "|Bakersfield|   CA|    USA| BFL| 39.37980769230769|      387|Delayed|\n",
      "|  Baltimore|   MD|    USA| BWI|31.533981362467866|     1288|Delayed|\n",
      "|     Bangor|   ME|    USA| BGR|             70.94|      555|Delayed|\n",
      "|     Barrow|   AK|    USA| BRW|29.064102564102566|      179|On-time|\n",
      "|Baton Rouge|   LA|    USA| BTR| 44.26229508196721|      490|Delayed|\n",
      "|   Beaumont|   TX|    USA| BPT|            43.625|      328|Delayed|\n",
      "+-----------+-----+-------+----+------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgDelay.filter(col('delay_mean')>0).withColumn('Status', expr(\"CASE WHEN delay_mean <= 30 THEN 'On-time' ELSE 'Delayed' END\")).show() # how to do this without the hard-coded string expression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coral-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-29 12:45:27.028735\n",
      "0:00:09.064761\n"
     ]
    }
   ],
   "source": [
    "# time if shows is false 0:00:42; time if shows is true 0:00.47\n",
    "stp = dt.datetime.now()\n",
    "print(stp)\n",
    "print(stp-stt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "northern-piano",
   "metadata": {},
   "source": [
    "Shows off = 9.1s\n",
    "Shows on  = 37.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "geological-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-framing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
