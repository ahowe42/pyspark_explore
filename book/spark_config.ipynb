{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sixth-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *#avg, count, expr\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "respective-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intimate-recovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '57647'),\n",
       " ('spark.app.id', 'local-1616082516175'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.host', 'EDI5M2HK13.woodmac.com')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surgical-investing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                        |value                                                           |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes            |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.coalescePartitions.enabled              |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum  |<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionNum      |<undefined>                                                     |\n",
      "|spark.sql.adaptive.enabled                                 |false                                                           |\n",
      "|spark.sql.adaptive.localShuffleReader.enabled              |true                                                            |\n",
      "|spark.sql.adaptive.skewJoin.enabled                        |true                                                            |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionFactor          |5                                                               |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes|256MB                                                           |\n",
      "|spark.sql.ansi.enabled                                     |false                                                           |\n",
      "|spark.sql.autoBroadcastJoinThreshold                       |10MB                                                            |\n",
      "|spark.sql.avro.compression.codec                           |snappy                                                          |\n",
      "|spark.sql.avro.deflate.level                               |-1                                                              |\n",
      "|spark.sql.broadcastTimeout                                 |300                                                             |\n",
      "|spark.sql.catalog.spark_catalog                            |<undefined>                                                     |\n",
      "|spark.sql.cbo.enabled                                      |false                                                           |\n",
      "|spark.sql.cbo.joinReorder.dp.star.filter                   |false                                                           |\n",
      "|spark.sql.cbo.joinReorder.dp.threshold                     |12                                                              |\n",
      "|spark.sql.cbo.joinReorder.enabled                          |false                                                           |\n",
      "|spark.sql.cbo.planStats.enabled                            |false                                                           |\n",
      "|spark.sql.cbo.starSchemaDetection                          |false                                                           |\n",
      "|spark.sql.columnNameOfCorruptRecord                        |_corrupt_record                                                 |\n",
      "|spark.sql.csv.filterPushdown.enabled                       |true                                                            |\n",
      "|spark.sql.datetime.java8API.enabled                        |false                                                           |\n",
      "|spark.sql.debug.maxToStringFields                          |25                                                              |\n",
      "|spark.sql.defaultCatalog                                   |spark_catalog                                                   |\n",
      "|spark.sql.event.truncate.length                            |2147483647                                                      |\n",
      "|spark.sql.execution.arrow.enabled                          |false                                                           |\n",
      "|spark.sql.execution.arrow.fallback.enabled                 |true                                                            |\n",
      "|spark.sql.execution.arrow.maxRecordsPerBatch               |10000                                                           |\n",
      "|spark.sql.execution.arrow.pyspark.enabled                  |<value of spark.sql.execution.arrow.enabled>                    |\n",
      "|spark.sql.execution.arrow.pyspark.fallback.enabled         |<value of spark.sql.execution.arrow.fallback.enabled>           |\n",
      "|spark.sql.execution.arrow.sparkr.enabled                   |false                                                           |\n",
      "|spark.sql.execution.pandas.udf.buffer.size                 |<value of spark.buffer.size>                                    |\n",
      "|spark.sql.extensions                                       |<undefined>                                                     |\n",
      "|spark.sql.files.ignoreCorruptFiles                         |false                                                           |\n",
      "|spark.sql.files.ignoreMissingFiles                         |false                                                           |\n",
      "|spark.sql.files.maxPartitionBytes                          |128MB                                                           |\n",
      "|spark.sql.files.maxRecordsPerFile                          |0                                                               |\n",
      "|spark.sql.function.concatBinaryAsString                    |false                                                           |\n",
      "|spark.sql.function.eltOutputAsString                       |false                                                           |\n",
      "|spark.sql.groupByAliases                                   |true                                                            |\n",
      "|spark.sql.groupByOrdinal                                   |true                                                            |\n",
      "|spark.sql.hive.filesourcePartitionFileCacheSize            |262144000                                                       |\n",
      "|spark.sql.hive.manageFilesourcePartitions                  |true                                                            |\n",
      "|spark.sql.hive.metastorePartitionPruning                   |true                                                            |\n",
      "|spark.sql.hive.thriftServer.singleSession                  |false                                                           |\n",
      "|spark.sql.hive.verifyPartitionPath                         |false                                                           |\n",
      "|spark.sql.inMemoryColumnarStorage.batchSize                |10000                                                           |\n",
      "|spark.sql.inMemoryColumnarStorage.compressed               |true                                                            |\n",
      "|spark.sql.inMemoryColumnarStorage.enableVectorizedReader   |true                                                            |\n",
      "|spark.sql.jsonGenerator.ignoreNullFields                   |true                                                            |\n",
      "|spark.sql.legacy.allowHashOnMapType                        |false                                                           |\n",
      "|spark.sql.legacy.sessionInitWithConfigDefaults             |false                                                           |\n",
      "|spark.sql.mapKeyDedupPolicy                                |EXCEPTION                                                       |\n",
      "|spark.sql.maven.additionalRemoteRepositories               |https://maven-central.storage-download.googleapis.com/maven2/   |\n",
      "|spark.sql.maxPlanStringLength                              |2147483632                                                      |\n",
      "|spark.sql.optimizer.dynamicPartitionPruning.enabled        |true                                                            |\n",
      "|spark.sql.optimizer.excludedRules                          |<undefined>                                                     |\n",
      "|spark.sql.orc.columnarReaderBatchSize                      |4096                                                            |\n",
      "|spark.sql.orc.compression.codec                            |snappy                                                          |\n",
      "|spark.sql.orc.enableVectorizedReader                       |true                                                            |\n",
      "|spark.sql.orc.filterPushdown                               |true                                                            |\n",
      "|spark.sql.orc.mergeSchema                                  |false                                                           |\n",
      "|spark.sql.orderByOrdinal                                   |true                                                            |\n",
      "|spark.sql.parquet.binaryAsString                           |false                                                           |\n",
      "|spark.sql.parquet.columnarReaderBatchSize                  |4096                                                            |\n",
      "|spark.sql.parquet.compression.codec                        |snappy                                                          |\n",
      "|spark.sql.parquet.enableVectorizedReader                   |true                                                            |\n",
      "|spark.sql.parquet.filterPushdown                           |true                                                            |\n",
      "|spark.sql.parquet.int96AsTimestamp                         |true                                                            |\n",
      "|spark.sql.parquet.int96TimestampConversion                 |false                                                           |\n",
      "|spark.sql.parquet.mergeSchema                              |false                                                           |\n",
      "|spark.sql.parquet.outputTimestampType                      |INT96                                                           |\n",
      "|spark.sql.parquet.recordLevelFilter.enabled                |false                                                           |\n",
      "|spark.sql.parquet.respectSummaryFiles                      |false                                                           |\n",
      "|spark.sql.parquet.writeLegacyFormat                        |false                                                           |\n",
      "|spark.sql.parser.quotedRegexColumnNames                    |false                                                           |\n",
      "|spark.sql.pivotMaxValues                                   |10000                                                           |\n",
      "|spark.sql.pyspark.jvmStacktrace.enabled                    |false                                                           |\n",
      "|spark.sql.queryExecutionListeners                          |<undefined>                                                     |\n",
      "|spark.sql.redaction.options.regex                          |(?i)url                                                         |\n",
      "|spark.sql.redaction.string.regex                           |<value of spark.redaction.string.regex>                         |\n",
      "|spark.sql.repl.eagerEval.enabled                           |false                                                           |\n",
      "|spark.sql.repl.eagerEval.maxNumRows                        |20                                                              |\n",
      "|spark.sql.repl.eagerEval.truncate                          |20                                                              |\n",
      "|spark.sql.session.timeZone                                 |Europe/London                                                   |\n",
      "|spark.sql.shuffle.partitions                               |200                                                             |\n",
      "|spark.sql.sources.bucketing.enabled                        |true                                                            |\n",
      "|spark.sql.sources.bucketing.maxBuckets                     |100000                                                          |\n",
      "|spark.sql.sources.default                                  |parquet                                                         |\n",
      "|spark.sql.sources.parallelPartitionDiscovery.threshold     |32                                                              |\n",
      "|spark.sql.sources.partitionColumnTypeInference.enabled     |true                                                            |\n",
      "|spark.sql.sources.partitionOverwriteMode                   |STATIC                                                          |\n",
      "|spark.sql.statistics.fallBackToHdfs                        |false                                                           |\n",
      "|spark.sql.statistics.histogram.enabled                     |false                                                           |\n",
      "|spark.sql.statistics.size.autoUpdate.enabled               |false                                                           |\n",
      "|spark.sql.storeAssignmentPolicy                            |ANSI                                                            |\n",
      "|spark.sql.streaming.checkpointLocation                     |<undefined>                                                     |\n",
      "|spark.sql.streaming.continuous.epochBacklogQueueSize       |10000                                                           |\n",
      "|spark.sql.streaming.disabledV2Writers                      |                                                                |\n",
      "|spark.sql.streaming.fileSource.cleaner.numThreads          |1                                                               |\n",
      "|spark.sql.streaming.forceDeleteTempCheckpointLocation      |false                                                           |\n",
      "|spark.sql.streaming.metricsEnabled                         |false                                                           |\n",
      "|spark.sql.streaming.multipleWatermarkPolicy                |min                                                             |\n",
      "|spark.sql.streaming.noDataMicroBatches.enabled             |true                                                            |\n",
      "|spark.sql.streaming.numRecentProgressUpdates               |100                                                             |\n",
      "|spark.sql.streaming.stopActiveRunOnRestart                 |true                                                            |\n",
      "|spark.sql.streaming.stopTimeout                            |0                                                               |\n",
      "|spark.sql.streaming.streamingQueryListeners                |<undefined>                                                     |\n",
      "|spark.sql.streaming.ui.enabled                             |true                                                            |\n",
      "|spark.sql.streaming.ui.retainedProgressUpdates             |100                                                             |\n",
      "|spark.sql.streaming.ui.retainedQueries                     |100                                                             |\n",
      "|spark.sql.thriftserver.scheduler.pool                      |<undefined>                                                     |\n",
      "|spark.sql.thriftserver.ui.retainedSessions                 |200                                                             |\n",
      "|spark.sql.thriftserver.ui.retainedStatements               |200                                                             |\n",
      "|spark.sql.ui.retainedExecutions                            |1000                                                            |\n",
      "|spark.sql.variable.substitute                              |true                                                            |\n",
      "|spark.sql.warehouse.dir                                    |file:/C:/Users/howean/Documents/spark_learning/spark-warehouse/ |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"SET -v\").select(\"key\", \"value\").show(n=1000, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-marathon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-increase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "curious-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-quebec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
